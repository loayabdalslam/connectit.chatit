Metadata-Version: 2.4
Name: connectit
Version: 0.1.0
Summary: Decentralized layer-parallel model training/hosting (prototype)
Author: connectit
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: typer>=0.12
Requires-Dist: rich>=13.7
Requires-Dist: websockets>=12.0
Requires-Dist: psutil>=5.9
Requires-Dist: numpy>=1.24
Provides-Extra: hf
Requires-Dist: transformers>=4.40; extra == "hf"
Requires-Dist: datasets>=2.17; extra == "hf"
Provides-Extra: onnx
Requires-Dist: onnx>=1.14; extra == "onnx"
Requires-Dist: onnxruntime>=1.17; extra == "onnx"
Provides-Extra: torch
Requires-Dist: torch>=2.1; (platform_system != "Windows" or platform_machine != "ARM64") and extra == "torch"
Provides-Extra: dht
Requires-Dist: kademlia>=2.2.2; extra == "dht"
Provides-Extra: nat
Requires-Dist: miniupnpc>=2.2.5; extra == "nat"
Requires-Dist: aiortc>=1.9.0; extra == "nat"
Provides-Extra: all
Requires-Dist: transformers>=4.40; extra == "all"
Requires-Dist: datasets>=2.17; extra == "all"
Requires-Dist: onnx>=1.14; extra == "all"
Requires-Dist: onnxruntime>=1.17; extra == "all"
Requires-Dist: torch>=2.1; (platform_system != "Windows" or platform_machine != "ARM64") and extra == "all"
Requires-Dist: kademlia>=2.2.2; extra == "all"
Requires-Dist: miniupnpc>=2.2.5; extra == "all"
Requires-Dist: aiortc>=1.9.0; extra == "all"

ConnectIT
==========

A peer-to-peer network for deploying and accessing Hugging Face language models. ConnectIT allows you to deploy any Hugging Face model as a service on a decentralized network and request text generation from the cheapest/lowest-latency providers.

Quickstart
----------

Prereqs: Python 3.9+, `pip`.

1) Install ConnectIT:

   ```bash
   pip install -e .
   ```

   For full functionality with Hugging Face models:

   ```bash
   pip install -e .[all]
   ```

2) Deploy a Hugging Face model:

   ```bash
   python -m connectit deploy-hf --model distilgpt2 --price-per-token 0.002 --host 127.0.0.1 --port 4334
   ```

3) Request text generation from another terminal:

   ```bash
   connectit p2p_request "Hello world" --model distilgpt2 --bootstrap_link ws://SEED_HOST:4001
   ```

Commands
--------

### deploy-hf

Deploy a Hugging Face text-generation model as a service on the P2P network.

```bash
python -m connectit deploy-hf --model MODEL_NAME --price-per-token PRICE --host HOST --port PORT
```

**Parameters:**
- `--model`: Hugging Face model name (e.g., `distilgpt2`, `gpt2`, `microsoft/DialoGPT-medium`)
- `--price-per-token`: Price per output token (float, e.g., `0.002`)
- `--host`: Bind host address (default: `0.0.0.0`)
- `--port`: Bind port (default: `4001`)
- `--bootstrap_link`: Optional P2P bootstrap link to join existing network

**Examples:**

```bash
# Deploy GPT-2 model
python -m connectit deploy-hf --model gpt2 --price-per-token 0.001 --host 0.0.0.0 --port 4001

# Deploy DistilGPT-2 with custom pricing
python -m connectit deploy-hf --model distilgpt2 --price-per-token 0.002 --host 127.0.0.1 --port 4334

# Join existing network
python -m connectit deploy-hf --model microsoft/DialoGPT-medium --price-per-token 0.005 --bootstrap_link ws://seed.example.com:4001
```

### p2p_request

Request text generation from the P2P network. Automatically selects the cheapest/lowest-latency provider for the specified model.

```bash
connectit p2p_request "PROMPT_TEXT" --model MODEL_NAME --bootstrap_link BOOTSTRAP_LINK
```

**Parameters:**
- `PROMPT_TEXT`: The text prompt for generation (required)
- `--model`: Model name to request (default: `distilgpt2`)
- `--bootstrap_link`: Bootstrap link to join the network (required for discovery)
- `--max_new_tokens`: Maximum new tokens to generate (default: `32`)

**Examples:**

```bash
# Basic text generation
connectit p2p_request "Hello world" --model distilgpt2 --bootstrap_link ws://127.0.0.1:4001

# Longer generation
connectit p2p_request "The future of AI is" --model gpt2 --max_new_tokens 50 --bootstrap_link ws://seed.example.com:4001

# Conversational model
connectit p2p_request "How are you today?" --model microsoft/DialoGPT-medium --bootstrap_link ws://127.0.0.1:4001
```

Programmatic Usage
------------------

You can use ConnectIT programmatically in your Python scripts:

```python
import asyncio
from connectit.p2p_runtime import P2PNode

async def request_generation(prompt, model_name="distilgpt2", bootstrap_link=None):
    """Request text generation programmatically."""
    node = P2PNode(host="127.0.0.1", port=0)
    await node.start()
    
    if bootstrap_link:
        await node.connect_bootstrap(bootstrap_link)
    
    # Wait for provider discovery
    await asyncio.sleep(2)
    
    # Find the best provider
    best = node.pick_provider(model_name)
    if not best:
        print(f"No provider found for model: {model_name}")
        return None
    
    provider_id, _ = best
    result = await node.request_generation(
        provider_id, 
        prompt, 
        max_new_tokens=32, 
        model_name=model_name
    )
    
    await node.stop()
    return result

# Usage
result = asyncio.run(request_generation(
    "Hello world", 
    model_name="distilgpt2",
    bootstrap_link="ws://127.0.0.1:4001"
))
print(result)
```

### Script Integration Examples

**Batch Processing:**

```python
import asyncio
from connectit.p2p_runtime import P2PNode

async def batch_generate(prompts, model_name="distilgpt2", bootstrap_link=None):
    """Generate text for multiple prompts."""
    node = P2PNode(host="127.0.0.1", port=0)
    await node.start()
    
    if bootstrap_link:
        await node.connect_bootstrap(bootstrap_link)
    await asyncio.sleep(2)  # Discovery time
    
    results = []
    for prompt in prompts:
        best = node.pick_provider(model_name)
        if best:
            provider_id, _ = best
            result = await node.request_generation(provider_id, prompt, model_name=model_name)
            results.append({"prompt": prompt, "result": result})
        else:
            results.append({"prompt": prompt, "result": None})
    
    await node.stop()
    return results

# Usage
prompts = ["Hello", "How are you?", "Tell me a story"]
results = asyncio.run(batch_generate(prompts, bootstrap_link="ws://127.0.0.1:4001"))
for item in results:
    print(f"Prompt: {item['prompt']}")
    print(f"Result: {item['result']}")
    print("---")
```

**Web Service Integration:**

```python
from flask import Flask, request, jsonify
import asyncio
from connectit.p2p_runtime import P2PNode

app = Flask(__name__)

@app.route('/generate', methods=['POST'])
def generate_text():
    data = request.json
    prompt = data.get('prompt')
    model = data.get('model', 'distilgpt2')
    bootstrap_link = data.get('bootstrap_link')
    
    async def _generate():
        node = P2PNode(host="127.0.0.1", port=0)
        await node.start()
        if bootstrap_link:
            await node.connect_bootstrap(bootstrap_link)
        await asyncio.sleep(2)
        
        best = node.pick_provider(model)
        if not best:
            return None
        
        provider_id, _ = best
        result = await node.request_generation(provider_id, prompt, model_name=model)
        await node.stop()
        return result
    
    result = asyncio.run(_generate())
    return jsonify({'result': result})

if __name__ == '__main__':
    app.run(debug=True)
```

Deploying Hugging Face Models
-----------------------------

### Supported Models

ConnectIT supports any Hugging Face Causal Language Model. Popular choices include:

- **GPT-2 family**: `gpt2`, `gpt2-medium`, `gpt2-large`, `gpt2-xl`
- **DistilGPT-2**: `distilgpt2` (smaller, faster)
- **DialoGPT**: `microsoft/DialoGPT-small`, `microsoft/DialoGPT-medium`, `microsoft/DialoGPT-large`
- **CodeGPT**: `microsoft/CodeGPT-small-py`
- **GPT-Neo**: `EleutherAI/gpt-neo-125M`, `EleutherAI/gpt-neo-1.3B`
- **Custom models**: Any compatible model from Hugging Face Hub

### Model Deployment Best Practices

1. **Choose appropriate pricing**: Set `--price-per-token` based on model size and computational cost
2. **Resource considerations**: Larger models require more memory and compute time
3. **Network setup**: Ensure your host/port is accessible to other network participants
4. **Model caching**: First deployment will download the model; subsequent runs use cached version

### Advanced Deployment

**Custom model with specific configuration:**

```bash
# Deploy a larger model with higher pricing
python -m connectit deploy-hf \
  --model EleutherAI/gpt-neo-1.3B \
  --price-per-token 0.01 \
  --host 0.0.0.0 \
  --port 4001 \
  --bootstrap_link ws://bootstrap.mynetwork.com:4001
```

**Multiple model deployment:**

You can run multiple instances on different ports to serve different models:

```bash
# Terminal 1: Deploy DistilGPT-2
python -m connectit deploy-hf --model distilgpt2 --price-per-token 0.001 --port 4001

# Terminal 2: Deploy GPT-2 Medium
python -m connectit deploy-hf --model gpt2-medium --price-per-token 0.005 --port 4002

# Terminal 3: Deploy DialoGPT
python -m connectit deploy-hf --model microsoft/DialoGPT-medium --price-per-token 0.003 --port 4003
```

Troubleshooting
---------------

**Common Issues:**

1. **Command not found**: Use `python -m connectit` instead of `connectit` if the command is not in PATH
2. **Model download fails**: Ensure internet connection and sufficient disk space
3. **No providers found**: Check bootstrap_link and ensure at least one provider is running
4. **Port conflicts**: Use different ports for multiple deployments
5. **Memory issues**: Use smaller models like `distilgpt2` for limited resources

**Dependencies:**

- Core functionality: `typer`, `rich`, `websockets`, `numpy`
- Hugging Face models: `transformers`, `torch`
- Full features: Install with `pip install -e .[all]`

**Performance Tips:**

- Use GPU-enabled PyTorch for faster inference on compatible hardware
- Choose model size based on available system resources
- Consider network latency when selecting bootstrap peers
- Monitor system resources during model deployment

License
-------

This is a prototype implementation. See license file for details.
